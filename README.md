# Интеллектуальный чат поддержки (прототип)
**Чтобы запустить проект с помощью Docker Compose, нужно:**

<p>1) Скачать проект из репозитория.</p>

```
git clone https://github.com/alexrink96/rosatom_support_agent
```

2) Скачать SLM-модель (при переходе по ссылке начнется загрузка): https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf?download=true

Если загрузка не началась, необходимо вручную скачать файл под названием "mistral-7b-instruct-v0.2.Q4_K_M.gguf" по ссылке: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/tree/main

3) Переместить скачанную модель в папку (важно не менять название модели, оставить то, что было при скачивании): fastapi_model/model/

4) Открыть BASH. Перейти в корневую папку проекта и ввести:

```
docker compose up --build
```

**Примечание 1:** если вы запускаете проект впервые, сборка может занять несколько минут — Docker скачает необходимые зависимости и создаст образы.

**Примечание 2:** после запусков контейнеров должно пройти пару минут, чтобы языковая модель начала работу.

После сборки:

Flask-сервер будет доступен по адресу: http://localhost:5001

SLM-сервер (FastAPI) — по адресу: http://localhost:8000/docs#/ (по этой ссылке можно отправить POST-запрос языковой модели напрямую и получить ответ)
